#!/usr/bin/env python3
"""
æ¨ç†è„šæœ¬èµ„æºè¯„æµ‹å·¥å…·
è¯„æµ‹æŒ‡æ ‡ï¼š
- CPUä½¿ç”¨ç‡
- GPUä½¿ç”¨ç‡ï¼ˆåˆ©ç”¨ç‡ã€æ˜¾å­˜å ç”¨ï¼‰
- å†…å­˜å ç”¨ï¼ˆRSS, VMSï¼‰
- æ¨ç†æ—¶é—´ï¼ˆæ€»æ—¶é—´ã€å¹³å‡æ¯å¸§æ—¶é—´ï¼‰
- FPSï¼ˆå¸§ç‡ï¼‰
"""

import subprocess
import psutil
import time
import os
import sys
import json
import threading
from datetime import datetime
from pathlib import Path


class InferenceBenchmark:
    """æ¨ç†è„šæœ¬èµ„æºè¯„æµ‹å·¥å…·"""
    
    def __init__(self, script_path, config_path, checkpoint_path, 
                 gpu_id=0, additional_args=None, sample_interval=0.1, debug=False):
        """
        Args:
            script_path: æ¨ç†è„šæœ¬è·¯å¾„ï¼ˆå¦‚ tools/test.pyï¼‰
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            gpu_id: GPU IDï¼ˆé»˜è®¤0ï¼‰
            additional_args: é¢å¤–çš„å‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨ï¼ˆå¦‚ ['--eval', 'bbox']ï¼‰
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.1ç§’
        """
        self.script_path = script_path
        self.config_path = config_path
        self.checkpoint_path = checkpoint_path
        self.gpu_id = gpu_id
        self.additional_args = additional_args or []
        self.sample_interval = sample_interval
        self.debug = debug
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(script_path):
            raise FileNotFoundError(f"æ¨ç†è„šæœ¬ä¸å­˜åœ¨: {script_path}")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
        self.gpu_available = self._check_gpu_available()
        if not self.gpu_available:
            print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–nvidia-smiä¸å¯ç”¨ï¼Œå°†æ— æ³•ç›‘æ§GPUä½¿ç”¨æƒ…å†µ")
    
    def _check_gpu_available(self):
        """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=index', '--format=csv,noheader'],
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                gpu_indices = [int(x.strip()) for x in result.stdout.strip().split('\n') if x.strip()]
                if self.gpu_id not in gpu_indices:
                    print(f"âš ï¸  è­¦å‘Š: GPU {self.gpu_id} ä¸å­˜åœ¨ï¼Œå¯ç”¨GPU: {gpu_indices}")
                    if gpu_indices:
                        self.gpu_id = gpu_indices[0]
                        print(f"   å°†ä½¿ç”¨GPU {self.gpu_id}")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError, ValueError):
            pass
        return False
    
    def get_gpu_stats(self, process_pid=None):
        """è·å–GPUä½¿ç”¨ç»Ÿè®¡"""
        if not self.gpu_available:
            return None
        
        try:
            # æŸ¥è¯¢GPUä½¿ç”¨ç‡å’Œæ˜¾å­˜ï¼ˆä½¿ç”¨æ›´è¯¦ç»†çš„æŸ¥è¯¢ï¼‰
            # æ³¨æ„ï¼šutilization.gpuæ˜¯è¿‡å»1ç§’çš„å¹³å‡å€¼ï¼Œå¯èƒ½ä¸å¤Ÿå®æ—¶
            # æˆ‘ä»¬åŒæ—¶æŸ¥è¯¢å¤šä¸ªæŒ‡æ ‡ä»¥è·å¾—æ›´å‡†ç¡®çš„ä¿¡æ¯
            query = f'--query-gpu=index,utilization.gpu,utilization.memory,memory.used,memory.total,power.draw,temperature.gpu'
            result = subprocess.run(
                ['nvidia-smi', query, '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=5, check=False
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    parts = [x.strip() for x in line.split(',')]
                    if len(parts) >= 6 and int(parts[0]) == self.gpu_id:
                        gpu_util = float(parts[1])
                        mem_util = float(parts[2])
                        mem_used = float(parts[3])
                        mem_total = float(parts[4])
                        power_draw = float(parts[5]) if len(parts) > 5 and parts[5] else 0
                        temp = float(parts[6]) if len(parts) > 6 and parts[6] else 0
                        mem_percent = (mem_used / mem_total) * 100 if mem_total > 0 else 0
                        
                        stats = {
                            'gpu_utilization_percent': gpu_util,
                            'gpu_memory_used_mb': mem_used,
                            'gpu_memory_total_mb': mem_total,
                            'gpu_memory_percent': mem_percent,
                            'gpu_memory_utilization_percent': mem_util
                        }
                        
                        # æ·»åŠ åŠŸè€—å’Œæ¸©åº¦ä¿¡æ¯ï¼ˆå¯ä»¥å¸®åŠ©åˆ¤æ–­GPUæ˜¯å¦åœ¨å·¥ä½œï¼‰
                        if power_draw > 0:
                            stats['gpu_power_watts'] = power_draw
                        if temp > 0:
                            stats['gpu_temperature_c'] = temp
                        
                        # å¦‚æœæä¾›äº†è¿›ç¨‹PIDï¼Œå°è¯•æŸ¥è¯¢è¯¥è¿›ç¨‹åŠå…¶å­è¿›ç¨‹çš„GPUä½¿ç”¨æƒ…å†µ
                        if process_pid is not None:
                            try:
                                # è·å–è¿›ç¨‹åŠå…¶æ‰€æœ‰å­è¿›ç¨‹çš„PID
                                pids_to_check = [process_pid]
                                try:
                                    proc = psutil.Process(process_pid)
                                    for child in proc.children(recursive=True):
                                        pids_to_check.append(child.pid)
                                except (psutil.NoSuchProcess, psutil.AccessDenied):
                                    pass
                                
                                # æŸ¥è¯¢æ‰€æœ‰GPUè¿›ç¨‹çš„ä½¿ç”¨æƒ…å†µ
                                proc_query = '--query-compute-apps=pid,process_name,used_memory --format=csv,noheader,nounits'
                                proc_result = subprocess.run(
                                    ['nvidia-smi', proc_query],
                                    capture_output=True, text=True, timeout=3, check=False
                                )
                                if proc_result.returncode == 0:
                                    total_proc_mem = 0
                                    proc_count = 0
                                    # æŸ¥æ‰¾åŒ¹é…çš„è¿›ç¨‹
                                    for proc_line in proc_result.stdout.strip().split('\n'):
                                        if proc_line.strip():
                                            proc_parts = [x.strip() for x in proc_line.split(',')]
                                            if len(proc_parts) >= 2:
                                                try:
                                                    pid = int(proc_parts[0])
                                                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è¿›ç¨‹æˆ–å…¶å­è¿›ç¨‹
                                                    if pid in pids_to_check:
                                                        proc_count += 1
                                                        if len(proc_parts) >= 3:
                                                            proc_mem = float(proc_parts[2])
                                                            total_proc_mem += proc_mem
                                                except ValueError:
                                                    continue
                                    if proc_count > 0:
                                        stats['process_gpu_memory_mb'] = total_proc_mem
                                        stats['process_gpu_count'] = proc_count
                            except Exception:
                                pass  # å¿½ç•¥è¿›ç¨‹æŸ¥è¯¢é”™è¯¯
                        
                        return stats
        except (subprocess.TimeoutExpired, ValueError, IndexError):
            pass
        
        return None
    
    def get_process_stats(self, process):
        """è·å–è¿›ç¨‹èµ„æºä½¿ç”¨ç»Ÿè®¡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = process.cpu_percent(interval=0.01)
            
            # å†…å­˜ä½¿ç”¨
            mem_info = process.memory_info()
            rss_mb = mem_info.rss / 1024 / 1024  # Resident Set Size (MB)
            vms_mb = mem_info.vms / 1024 / 1024  # Virtual Memory Size (MB)
            
            # å†…å­˜ç™¾åˆ†æ¯”
            mem_percent = process.memory_percent()
            
            # è·å–å­è¿›ç¨‹ç»Ÿè®¡ï¼ˆPythonè„šæœ¬å¯èƒ½å¯åŠ¨å­è¿›ç¨‹ï¼‰
            children_stats = []
            try:
                for child in process.children(recursive=True):
                    try:
                        child_mem = child.memory_info()
                        children_stats.append({
                            'rss_mb': child_mem.rss / 1024 / 1024,
                            'vms_mb': child_mem.vms / 1024 / 1024
                        })
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
            
            # è®¡ç®—å­è¿›ç¨‹æ€»å†…å­˜
            children_rss = sum(c['rss_mb'] for c in children_stats)
            children_vms = sum(c['vms_mb'] for c in children_stats)
            
            stats = {
                'cpu_percent': cpu_percent,
                'memory_rss_mb': rss_mb,
                'memory_vms_mb': vms_mb,
                'memory_percent': mem_percent,
                'children_count': len(children_stats),
                'children_rss_mb': children_rss,
                'children_vms_mb': children_vms,
                'total_rss_mb': rss_mb + children_rss,
                'total_vms_mb': vms_mb + children_vms
            }
            
            # æ·»åŠ GPUç»Ÿè®¡ï¼ˆä¼ å…¥è¿›ç¨‹PIDä»¥ä¾¿æ›´ç²¾ç¡®ç›‘æ§ï¼‰
            gpu_stats = self.get_gpu_stats(process.pid)
            if gpu_stats:
                stats.update(gpu_stats)
            
            return stats
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            return None
    
    def run_benchmark(self):
        """è¿è¡Œæ€§èƒ½è¯„æµ‹"""
        print("=" * 70)
        print("æ¨ç†è„šæœ¬èµ„æºè¯„æµ‹")
        print("=" * 70)
        print(f"æ¨ç†è„šæœ¬: {self.script_path}")
        print(f"é…ç½®æ–‡ä»¶: {self.config_path}")
        print(f"æ£€æŸ¥ç‚¹æ–‡ä»¶: {self.checkpoint_path}")
        print(f"GPU ID: {self.gpu_id}")
        if self.additional_args:
            print(f"é¢å¤–å‚æ•°: {' '.join(self.additional_args)}")
        print(f"é‡‡æ ·é—´éš”: {self.sample_interval * 1000:.1f} ms")
        print("-" * 70)
        
        # æ„å»ºå‘½ä»¤ - ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
        python_exe = sys.executable
        # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
        script_abs = os.path.abspath(self.script_path)
        config_abs = os.path.abspath(self.config_path)
        checkpoint_abs = os.path.abspath(self.checkpoint_path)
        
        # å¯¹äºå•GPUï¼Œç›´æ¥è¿è¡Œtest.pyï¼ˆå·²ä¿®æ”¹æ”¯æŒéåˆ†å¸ƒå¼æ¨¡å¼ï¼‰
        # ä¸ä½¿ç”¨ torch.distributed.launch
        cmd = [python_exe, script_abs, config_abs, checkpoint_abs]
        cmd.extend(self.additional_args)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(self.gpu_id)
        
        # è®¡ç®—å·¥ä½œç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
        # ä»è„šæœ¬è·¯å¾„å‘ä¸ŠæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å« README.md æˆ– setup.py çš„ç›®å½•ï¼‰
        script_dir = os.path.dirname(script_abs)
        work_dir = script_dir
        current_dir = script_dir
        
        # å‘ä¸ŠæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
        for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5çº§
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å« README.md, setup.py, æˆ– projects/ ç›®å½•ï¼‰
            if (os.path.exists(os.path.join(current_dir, 'README.md')) or
                os.path.exists(os.path.join(current_dir, 'README_zh.md')) or
                os.path.exists(os.path.join(current_dir, 'setup.py')) or
                os.path.exists(os.path.join(current_dir, 'projects'))):
                work_dir = current_dir
                break
            parent = os.path.dirname(current_dir)
            if parent == current_dir:  # å·²åˆ°æ ¹ç›®å½•
                break
            current_dir = parent
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆå‡è®¾è„šæœ¬åœ¨ tools/ ä¸‹ï¼‰
        if work_dir == script_dir and os.path.basename(script_dir) == 'tools':
            work_dir = os.path.dirname(script_dir)
        
        # è®¾ç½®PYTHONPATHä»¥ä¾¿æ‰¾åˆ°mmdet3dæ¨¡å—
        # é¡¹ç›®æ ¹ç›®å½•éœ€è¦æ·»åŠ åˆ°PYTHONPATHä¸­
        project_root = work_dir
        current_pythonpath = env.get('PYTHONPATH', '')
        if current_pythonpath:
            # å¦‚æœPYTHONPATHå·²å­˜åœ¨ï¼Œå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°å‰é¢
            env['PYTHONPATH'] = f"{project_root}:{current_pythonpath}"
        else:
            env['PYTHONPATH'] = project_root
        
        # è®°å½•GPUåŸºçº¿çŠ¶æ€ï¼ˆåœ¨å¯åŠ¨è¿›ç¨‹ä¹‹å‰ï¼‰
        gpu_baseline = None
        if self.gpu_available:
            print("æ­£åœ¨è®°å½•GPUåŸºçº¿çŠ¶æ€...")
            time.sleep(0.5)  # ç­‰å¾…0.5ç§’ç¡®ä¿GPUçŠ¶æ€ç¨³å®š
            baseline_stats = self.get_gpu_stats()
            if baseline_stats:
                gpu_baseline = {
                    'memory_used_mb': baseline_stats.get('gpu_memory_used_mb', 0),
                    'utilization_percent': baseline_stats.get('gpu_utilization_percent', 0),
                    'power_watts': baseline_stats.get('gpu_power_watts', 0),
                    'temperature_c': baseline_stats.get('gpu_temperature_c', 0)
                }
                print(f"GPUåŸºçº¿çŠ¶æ€: æ˜¾å­˜={gpu_baseline['memory_used_mb']:.0f}MB, "
                      f"åˆ©ç”¨ç‡={gpu_baseline['utilization_percent']:.1f}%")
        
        # å¯åŠ¨è¿›ç¨‹
        start_time = time.time()
        try:
            process = psutil.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                cwd=work_dir
            )
        except Exception as e:
            raise RuntimeError(f"å¯åŠ¨æ¨ç†è„šæœ¬å¤±è´¥: {e}") from e
        
        # ç›‘æ§èµ„æºä½¿ç”¨
        stats_list = []
        monitoring = True
        
        def monitor_loop():
            """ç›‘æ§å¾ªç¯ï¼ˆåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
            nonlocal monitoring
            # ç«‹å³å¼€å§‹ç›‘æ§ï¼Œä¸ç­‰å¾…ï¼ˆç¡®ä¿æ•è·GPUä½¿ç”¨çš„åˆå§‹é˜¶æ®µï¼‰
            sample_count = 0
            
            while monitoring and process.poll() is None:
                stats = self.get_process_stats(process)
                if stats:
                    # è®¡ç®—GPUæ˜¾å­˜å¢é‡ï¼ˆå‡å»åŸºçº¿ï¼‰
                    if gpu_baseline and 'gpu_memory_used_mb' in stats:
                        baseline_mem = gpu_baseline.get('memory_used_mb', 0)
                        current_mem = stats.get('gpu_memory_used_mb', 0)
                        stats['gpu_memory_increment_mb'] = max(0, current_mem - baseline_mem)
                    
                    # è®¡ç®—GPUåˆ©ç”¨ç‡å¢é‡ï¼ˆå‡å»åŸºçº¿ï¼‰
                    if gpu_baseline and 'gpu_utilization_percent' in stats:
                        baseline_util = gpu_baseline.get('utilization_percent', 0)
                        current_util = stats.get('gpu_utilization_percent', 0)
                        stats['gpu_utilization_increment_percent'] = max(0, current_util - baseline_util)
                    
                    stats['timestamp'] = time.time() - start_time
                    stats_list.append(stats)
                    sample_count += 1
                    
                    # è°ƒè¯•æ¨¡å¼ï¼šå®æ—¶æ‰“å°GPUä½¿ç”¨æƒ…å†µ
                    if self.debug and 'gpu_utilization_percent' in stats:
                        gpu_util = stats.get('gpu_utilization_percent', 0)
                        gpu_mem = stats.get('gpu_memory_used_mb', 0)
                        gpu_mem_inc = stats.get('gpu_memory_increment_mb', 0)
                        gpu_util_inc = stats.get('gpu_utilization_increment_percent', 0)
                        print(f"[{stats['timestamp']:.2f}s] GPU: {gpu_util:.1f}% (+{gpu_util_inc:.1f}%) | "
                              f"æ˜¾å­˜: {gpu_mem:.0f}MB (+{gpu_mem_inc:.0f}MB)", flush=True)
                    
                    # å‰å‡ æ¬¡é‡‡æ ·ä½¿ç”¨æ›´çŸ­çš„é—´éš”ä»¥å¿«é€Ÿæ•è·GPUä½¿ç”¨å˜åŒ–
                    if sample_count <= 5:
                        time.sleep(0.01)  # å‰5æ¬¡é‡‡æ ·ä½¿ç”¨10msé—´éš”
                    elif sample_count <= 20:
                        time.sleep(0.02)  # æ¥ä¸‹æ¥15æ¬¡ä½¿ç”¨20msé—´éš”
                    else:
                        time.sleep(self.sample_interval)  # ä¹‹åä½¿ç”¨æ­£å¸¸é—´éš”
                else:
                    time.sleep(self.sample_interval)
            
            # è·å–æœ€åä¸€æ¬¡ç»Ÿè®¡
            if monitoring:
                stats = self.get_process_stats(process)
                if stats:
                    # è®¡ç®—GPUæ˜¾å­˜å¢é‡ï¼ˆå‡å»åŸºçº¿ï¼‰
                    if gpu_baseline and 'gpu_memory_used_mb' in stats:
                        baseline_mem = gpu_baseline.get('memory_used_mb', 0)
                        current_mem = stats.get('gpu_memory_used_mb', 0)
                        stats['gpu_memory_increment_mb'] = max(0, current_mem - baseline_mem)
                    
                    # è®¡ç®—GPUåˆ©ç”¨ç‡å¢é‡ï¼ˆå‡å»åŸºçº¿ï¼‰
                    if gpu_baseline and 'gpu_utilization_percent' in stats:
                        baseline_util = gpu_baseline.get('utilization_percent', 0)
                        current_util = stats.get('gpu_utilization_percent', 0)
                        stats['gpu_utilization_increment_percent'] = max(0, current_util - baseline_util)
                    
                    stats['timestamp'] = time.time() - start_time
                    stats_list.append(stats)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        
        try:
            # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶è·å–è¾“å‡º
            stdout, stderr = process.communicate()
            end_time = time.time()
            monitoring = False
            
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            monitor_thread.join(timeout=1.0)
            
        except KeyboardInterrupt:
            print("\nè¯„æµ‹è¢«ä¸­æ–­")
            monitoring = False
            process.terminate()
            try:
                process.wait(timeout=5)
            except psutil.TimeoutExpired:
                process.kill()
            sys.exit(1)
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = end_time - start_time
        
        # è§£æè¾“å‡ºè·å–å¤„ç†æ ·æœ¬æ•°
        output_text = stdout.decode('utf-8', errors='ignore')
        error_text = stderr.decode('utf-8', errors='ignore')
        processed_samples = self._parse_sample_count(output_text, error_text)
        
        # æ£€æµ‹è¿›ç¨‹æ˜¯å¦çœŸæ­£è¿è¡Œï¼ˆé€šè¿‡GPUä½¿ç”¨æƒ…å†µåˆ¤æ–­ï¼‰
        gpu_actually_used = False
        if gpu_baseline and stats_list:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„GPUä½¿ç”¨ï¼ˆåˆ©ç”¨ç‡å¢é‡>10%æˆ–æ˜¾å­˜å¢é‡>50MBï¼‰
            max_util_inc = max([s.get('gpu_utilization_increment_percent', 0) for s in stats_list], default=0)
            max_mem_inc = max([s.get('gpu_memory_increment_mb', 0) for s in stats_list], default=0)
            if max_util_inc > 10 or max_mem_inc > 50:
                gpu_actually_used = True
        
        # å¦‚æœè¿›ç¨‹é€€å‡ºç é0ä¸”GPUä½¿ç”¨å¾ˆå°‘ï¼Œè¯´æ˜è¿›ç¨‹å¯èƒ½æ²¡æœ‰çœŸæ­£è¿è¡Œ
        if process.returncode != 0 and not gpu_actually_used:
            print(f"\nâš ï¸  è­¦å‘Š: è¿›ç¨‹å¼‚å¸¸é€€å‡ºï¼ˆé€€å‡ºç : {process.returncode}ï¼‰ï¼Œä¸”GPUä½¿ç”¨å¾ˆå°‘")
            print(f"   è¿™è¡¨æ˜è¿›ç¨‹å¯èƒ½åœ¨å¯åŠ¨é˜¶æ®µå°±å¤±è´¥äº†ï¼Œæ²¡æœ‰çœŸæ­£è¿è¡Œæ¨ç†ä»»åŠ¡")
            print(f"   GPUåˆ©ç”¨ç‡å¢é‡: {max_util_inc:.1f}% (æ­£å¸¸æ¨ç†åº”è¯¥>20%)")
            print(f"   æ˜¾å­˜å¢é‡: {max_mem_inc:.0f}MB (æ­£å¸¸æ¨ç†åº”è¯¥>100MB)")
            if error_text:
                error_preview = error_text[-500:] if len(error_text) > 500 else error_text
                print(f"\n   é”™è¯¯ä¿¡æ¯é¢„è§ˆ:")
                for line in error_preview.split('\n')[-8:]:
                    if line.strip():
                        print(f"     {line}")
        
        # éªŒè¯ï¼šå¦‚æœè¿›ç¨‹è¿è¡Œå¤ªå¿«ï¼Œé‡‡æ ·å¯èƒ½ä¸å‡†ç¡®
        if total_time < 1.0 and len(stats_list) < 10:
            print(f"\nğŸ’¡ æç¤º: è¿›ç¨‹è¿è¡Œè¾ƒå¿« ({total_time:.3f}ç§’)ï¼Œé‡‡æ ·æ¬¡æ•°è¾ƒå°‘ ({len(stats_list)}æ¬¡)")
            print(f"   å»ºè®®: å¤„ç†æ›´å¤šæ ·æœ¬ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»Ÿè®¡é‡‡æ ·")
        
        # æ£€æµ‹å¸¸è§é”™è¯¯
        errors_detected = self._detect_errors(error_text, process.returncode)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        results = self._compute_statistics(stats_list, total_time, processed_samples, gpu_baseline)
        
        # æ·»åŠ è¿›ç¨‹è¿è¡ŒçŠ¶æ€ä¿¡æ¯
        results['process_status'] = {
            'return_code': process.returncode,
            'gpu_actually_used': gpu_actually_used,
            'total_time_seconds': total_time,
            'max_gpu_utilization_increment': max_util_inc if gpu_baseline else 0,
            'max_gpu_memory_increment_mb': max_mem_inc if gpu_baseline else 0
        }
        
        # ä¿å­˜åŸå§‹è¾“å‡º
        results['stdout'] = output_text
        results['stderr'] = error_text
        results['return_code'] = process.returncode
        if errors_detected:
            results['errors_detected'] = errors_detected
        
        return results
    
    def _parse_sample_count(self, output_text, error_text):
        """ä»è¾“å‡ºä¸­è§£æå¤„ç†çš„æ ·æœ¬æ•°"""
        import re
        
        combined_text = output_text + '\n' + error_text
        
        # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…æ ·æœ¬æ•°
        patterns = [
            # mmdet3d è¿›åº¦æ¡æ ¼å¼: [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 6019/6019
            r'\[(>+\s*)\]\s*(\d+)/(\d+)',
            # Done: [1234/1234]
            r'Done.*?\[(\d+)/',
            # Evaluating 6019 samples
            r'(?:Evaluating|Processing)\s+(\d+)\s+samples',
            # load 6019 samples
            r'load(?:ed)?\s+(\d+)\s+samples',
            # ä¸­æ–‡æ¨¡å¼
            r'å¤„ç†.*?(\d+).*?æ ·æœ¬',
            r'è¯„ä¼°.*?(\d+).*?æ ·æœ¬',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            if matches:
                try:
                    # å¤„ç†ä¸åŒçš„åŒ¹é…ç»„æ ¼å¼
                    if isinstance(matches[-1], tuple):
                        # å–å…ƒç»„ä¸­æœ€åä¸€ä¸ªéç©ºæ•°å­—
                        for num in reversed(matches[-1]):
                            if num and num.isdigit():
                                return int(num)
                    else:
                        return int(matches[-1])
                except (ValueError, AttributeError):
                    continue
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›0å¹¶ç»™å‡ºè­¦å‘Š
        if output_text or error_text:
            print(f"\nâš ï¸  è­¦å‘Š: æ— æ³•ä»è¾“å‡ºä¸­è§£æå¤„ç†æ ·æœ¬æ•°")
            print(f"   è¾“å‡ºé¢„è§ˆï¼ˆæœ€å300å­—ç¬¦ï¼‰: {(output_text + error_text)[-300:]}")
        
        return 0
    
    def _detect_errors(self, error_text, return_code):
        """æ£€æµ‹å¸¸è§é”™è¯¯å¹¶è¿”å›é”™è¯¯ä¿¡æ¯"""
        errors = []
        
        if return_code != 0:
            # æ£€æµ‹è¯„æµ‹æŒ‡æ ‡é”™è¯¯
            if 'metric' in error_text.lower() and 'not supported' in error_text.lower():
                import re
                metric_match = re.search(r"metric ([\w]+) is not supported", error_text)
                if metric_match:
                    wrong_metric = metric_match.group(1)
                    errors.append({
                        'type': 'invalid_metric',
                        'message': f'ä¸æ”¯æŒçš„è¯„æµ‹æŒ‡æ ‡: {wrong_metric}',
                        'description': 'MapTR æ˜¯åœ°å›¾é‡å»ºä»»åŠ¡ï¼Œä¸æ”¯æŒç›®æ ‡æ£€æµ‹çš„ bbox æŒ‡æ ‡',
                        'suggestions': [
                            'ä½¿ç”¨ chamfer æŒ‡æ ‡: --eval chamfer',
                            'æˆ–ä½¿ç”¨ iou æŒ‡æ ‡: --eval iou',
                            'æŸ¥çœ‹é…ç½®æ–‡ä»¶äº†è§£æ”¯æŒçš„è¯„æµ‹æŒ‡æ ‡'
                        ]
                    })
            
            # æ£€æµ‹æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯
            if 'FileNotFoundError' in error_text or 'No such file or directory' in error_text:
                import re
                # å°è¯•æå–æ–‡ä»¶è·¯å¾„
                match = re.search(r"(?:FileNotFoundError|No such file or directory)[:\s]+.*?['\"]([^'\"]+)['\"]", error_text)
                if match:
                    missing_file = match.group(1)
                    errors.append({
                        'type': 'file_not_found',
                        'message': f'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨',
                        'description': f'æ‰¾ä¸åˆ°æ–‡ä»¶: {missing_file}',
                        'suggestions': [
                            'æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½å¹¶æ”¾ç½®åœ¨æ­£ç¡®ä½ç½®',
                            'å¦‚æœæ˜¯ nuscenes æ•°æ®é›†ï¼Œè¯·å‚è€ƒ docs/prepare_dataset.md å‡†å¤‡æ•°æ®',
                            'æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®è·¯å¾„è®¾ç½®æ˜¯å¦æ­£ç¡®',
                            f'ç¡®ä¿æ–‡ä»¶å­˜åœ¨: ls -la {missing_file}'
                        ]
                    })
                else:
                    errors.append({
                        'type': 'file_not_found',
                        'message': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨',
                        'description': 'æ£€æµ‹åˆ°æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®é…ç½®',
                        'suggestions': [
                            'æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½å¹¶æ”¾ç½®åœ¨æ­£ç¡®ä½ç½®',
                            'å‚è€ƒ docs/prepare_dataset.md å‡†å¤‡æ•°æ®',
                            'æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®è·¯å¾„è®¾ç½®'
                        ]
                    })
            
            # æ£€æµ‹numbaé”™è¯¯
            if 'numba.errors' in error_text or 'ModuleNotFoundError' in error_text and 'numba' in error_text:
                errors.append({
                    'type': 'numba_compatibility',
                    'message': 'Numbaç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜',
                    'description': 'æ£€æµ‹åˆ°numba.errorså¯¼å…¥é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯numbaç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´çš„',
                    'suggestions': [
                        'æ£€æŸ¥numbaç‰ˆæœ¬: pip show numba',
                        'å°è¯•é™çº§numba: pip install numba==0.48.0',
                        'æˆ–è€…å‡çº§numbaåˆ°æœ€æ–°ç‰ˆæœ¬: pip install --upgrade numba',
                        'å¦‚æœä½¿ç”¨numba >= 0.57ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹ä»£ç : from numba import NumbaPerformanceWarning (è€Œä¸æ˜¯from numba.errors)'
                    ]
                })
            
            # æ£€æµ‹å…¶ä»–å¸¸è§é”™è¯¯
            if 'ModuleNotFoundError' in error_text and 'numba' not in error_text:
                import re
                match = re.search(r"ModuleNotFoundError: No module named '([^']+)'", error_text)
                if match:
                    module_name = match.group(1)
                    errors.append({
                        'type': 'missing_module',
                        'message': f'ç¼ºå°‘æ¨¡å—: {module_name}',
                        'suggestions': [f'å®‰è£…ç¼ºå¤±çš„æ¨¡å—: pip install {module_name}']
                    })
            
            if 'CUDA' in error_text or 'cuda' in error_text:
                errors.append({
                    'type': 'cuda_error',
                    'message': 'CUDAç›¸å…³é”™è¯¯',
                    'suggestions': [
                        'æ£€æŸ¥CUDAé©±åŠ¨: nvidia-smi',
                        'æ£€æŸ¥PyTorch CUDAæ”¯æŒ: python -c "import torch; print(torch.cuda.is_available())"',
                        'æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§'
                    ]
                })
            
            # æ£€æµ‹ç³»ç»Ÿåº“ç‰ˆæœ¬é”™è¯¯ï¼ˆlibstdc++, CXXABIç­‰ï¼‰
            if 'libstdc++' in error_text or 'CXXABI' in error_text or 'version `' in error_text:
                errors.append({
                    'type': 'system_library_error',
                    'message': 'ç³»ç»Ÿåº“ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜',
                    'description': 'æ£€æµ‹åˆ°ç³»ç»Ÿåº“ç‰ˆæœ¬ä¸å…¼å®¹é”™è¯¯ï¼ˆå¦‚libstdc++ã€CXXABIç­‰ï¼‰ï¼Œè¿™é€šå¸¸æ˜¯condaç¯å¢ƒä¸ç³»ç»Ÿåº“ç‰ˆæœ¬ä¸åŒ¹é…å¯¼è‡´çš„',
                    'suggestions': [
                        'å°è¯•ä½¿ç”¨condaç¯å¢ƒä¸­çš„libstdc++: conda install -c conda-forge libstdcxx-ng',
                        'æˆ–è€…æ›´æ–°ç³»ç»Ÿåº“: sudo apt-get update && sudo apt-get install libstdc++6',
                        'æ£€æŸ¥condaç¯å¢ƒ: conda list | grep libstdc',
                        'å¦‚æœä½¿ç”¨condaç¯å¢ƒï¼Œå°è¯•: conda update --all',
                        'æˆ–è€…è®¾ç½®LD_LIBRARY_PATHæŒ‡å‘condaç¯å¢ƒçš„libç›®å½•'
                    ]
                })
        
        return errors if errors else None
    
    def _compute_statistics(self, stats_list, total_time, processed_samples, gpu_baseline=None):
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        if not stats_list:
            return {
                'error': 'æœªèƒ½æ”¶é›†åˆ°æ€§èƒ½æ•°æ®',
                'total_time': total_time,
                'processed_samples': processed_samples
            }
        
        # CPUç»Ÿè®¡
        cpu_values = [s['cpu_percent'] for s in stats_list if 'cpu_percent' in s]
        cpu_mean = sum(cpu_values) / len(cpu_values) if cpu_values else 0
        cpu_max = max(cpu_values) if cpu_values else 0
        cpu_min = min(cpu_values) if cpu_values else 0
        
        # å†…å­˜ç»Ÿè®¡ (RSS)
        rss_values = [s.get('total_rss_mb', s.get('memory_rss_mb', 0)) for s in stats_list]
        rss_mean = sum(rss_values) / len(rss_values) if rss_values else 0
        rss_max = max(rss_values) if rss_values else 0
        rss_min = min(rss_values) if rss_values else 0
        
        # å†…å­˜ç»Ÿè®¡ (VMS)
        vms_values = [s.get('total_vms_mb', s.get('memory_vms_mb', 0)) for s in stats_list]
        vms_mean = sum(vms_values) / len(vms_values) if vms_values else 0
        vms_max = max(vms_values) if vms_values else 0
        
        # å†…å­˜ç™¾åˆ†æ¯”
        mem_percent_values = [s['memory_percent'] for s in stats_list if 'memory_percent' in s]
        mem_percent_mean = sum(mem_percent_values) / len(mem_percent_values) if mem_percent_values else 0
        mem_percent_max = max(mem_percent_values) if mem_percent_values else 0
        
        # GPUç»Ÿè®¡
        gpu_stats = {}
        if self.gpu_available:
            gpu_util_values = [s['gpu_utilization_percent'] for s in stats_list if 'gpu_utilization_percent' in s]
            gpu_mem_values = [s['gpu_memory_used_mb'] for s in stats_list if 'gpu_memory_used_mb' in s]
            gpu_mem_percent_values = [s['gpu_memory_percent'] for s in stats_list if 'gpu_memory_percent' in s]
            gpu_power_values = [s['gpu_power_watts'] for s in stats_list if 'gpu_power_watts' in s]
            gpu_temp_values = [s['gpu_temperature_c'] for s in stats_list if 'gpu_temperature_c' in s]
            
            # è®¡ç®—å¢é‡ï¼ˆå‡å»åŸºçº¿ï¼‰
            gpu_util_inc_values = [s.get('gpu_utilization_increment_percent', 0) for s in stats_list if 'gpu_utilization_increment_percent' in s]
            gpu_mem_inc_values = [s.get('gpu_memory_increment_mb', 0) for s in stats_list if 'gpu_memory_increment_mb' in s]
            
            if gpu_util_values:
                gpu_stats = {
                    'utilization': {
                        'mean_percent': round(sum(gpu_util_values) / len(gpu_util_values), 2),
                        'max_percent': round(max(gpu_util_values), 2),
                        'min_percent': round(min(gpu_util_values), 2),
                    },
                    'memory': {
                        'mean_mb': round(sum(gpu_mem_values) / len(gpu_mem_values), 2),
                        'max_mb': round(max(gpu_mem_values), 2),
                        'min_mb': round(min(gpu_mem_values), 2),
                    },
                    'memory_percent': {
                        'mean': round(sum(gpu_mem_percent_values) / len(gpu_mem_percent_values), 2),
                        'max': round(max(gpu_mem_percent_values), 2),
                    }
                }
                
                # æ·»åŠ å¢é‡ç»Ÿè®¡ï¼ˆå‡å»åŸºçº¿åçš„å®é™…ä½¿ç”¨ï¼‰
                if gpu_baseline:
                    gpu_stats['baseline'] = {
                        'memory_mb': round(gpu_baseline.get('memory_used_mb', 0), 2),
                        'utilization_percent': round(gpu_baseline.get('utilization_percent', 0), 2)
                    }
                    
                    if gpu_util_inc_values:
                        gpu_stats['utilization_increment'] = {
                            'mean_percent': round(sum(gpu_util_inc_values) / len(gpu_util_inc_values), 2),
                            'max_percent': round(max(gpu_util_inc_values), 2),
                            'min_percent': round(min(gpu_util_inc_values), 2),
                        }
                    
                    if gpu_mem_inc_values:
                        gpu_stats['memory_increment'] = {
                            'mean_mb': round(sum(gpu_mem_inc_values) / len(gpu_mem_inc_values), 2),
                            'max_mb': round(max(gpu_mem_inc_values), 2),
                            'min_mb': round(min(gpu_mem_inc_values), 2),
                        }
                
                # è·å–GPUæ€»æ˜¾å­˜
                if stats_list and 'gpu_memory_total_mb' in stats_list[0]:
                    gpu_stats['memory']['total_mb'] = round(stats_list[0]['gpu_memory_total_mb'], 2)
                
                # æ·»åŠ åŠŸè€—å’Œæ¸©åº¦ç»Ÿè®¡
                if gpu_power_values:
                    gpu_stats['power'] = {
                        'mean_watts': round(sum(gpu_power_values) / len(gpu_power_values), 2),
                        'max_watts': round(max(gpu_power_values), 2),
                        'min_watts': round(min(gpu_power_values), 2),
                    }
                    if gpu_baseline and 'power_watts' in gpu_baseline:
                        baseline_power = gpu_baseline.get('power_watts', 0)
                        gpu_stats['power']['baseline_watts'] = round(baseline_power, 2)
                        gpu_stats['power']['increment_mean_watts'] = round(
                            sum(gpu_power_values) / len(gpu_power_values) - baseline_power, 2)
                        gpu_stats['power']['increment_max_watts'] = round(
                            max(gpu_power_values) - baseline_power, 2)
                
                if gpu_temp_values:
                    gpu_stats['temperature'] = {
                        'mean_c': round(sum(gpu_temp_values) / len(gpu_temp_values), 2),
                        'max_c': round(max(gpu_temp_values), 2),
                        'min_c': round(min(gpu_temp_values), 2),
                    }
        
        # è®¡ç®—FPSå’Œæ—¶é—´
        if total_time > 0 and processed_samples > 0:
            fps = processed_samples / total_time
            time_per_sample = total_time / processed_samples
        else:
            fps = 0
            time_per_sample = 0
        
        results = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'config': {
                'script': self.script_path,
                'config': self.config_path,
                'checkpoint': self.checkpoint_path,
                'gpu_id': self.gpu_id,
                'additional_args': self.additional_args
            },
            'performance': {
                'total_time_seconds': round(total_time, 3),
                'processed_samples': processed_samples,
                'fps': round(fps, 2),
                'time_per_sample_ms': round(time_per_sample * 1000, 2),
            },
            'cpu': {
                'mean_percent': round(cpu_mean, 2),
                'max_percent': round(cpu_max, 2),
                'min_percent': round(cpu_min, 2),
            },
            'memory': {
                'rss': {
                    'mean_mb': round(rss_mean, 2),
                    'max_mb': round(rss_max, 2),
                    'min_mb': round(rss_min, 2),
                },
                'vms': {
                    'mean_mb': round(vms_mean, 2),
                    'max_mb': round(vms_max, 2),
                },
                'percent': {
                    'mean': round(mem_percent_mean, 2),
                    'max': round(mem_percent_max, 2),
                }
            },
            'samples': len(stats_list),
            'sample_interval_ms': round(self.sample_interval * 1000, 1)
        }
        
        # æ·»åŠ GPUç»Ÿè®¡
        if gpu_stats:
            results['gpu'] = gpu_stats
        
        return results
    
    def print_results(self, results):
        """æ‰“å°è¯„æµ‹ç»“æœ"""
        if 'error' in results:
            print(f"\né”™è¯¯: {results['error']}")
            return
        
        # æ£€æŸ¥è¿›ç¨‹è¿è¡ŒçŠ¶æ€
        if 'process_status' in results:
            status = results['process_status']
            if status['return_code'] != 0 and not status.get('gpu_actually_used', False):
                print("\n" + "=" * 70)
                print("âš ï¸  è¿›ç¨‹æœªæ­£å¸¸è¿è¡Œ")
                print("=" * 70)
                print(f"  é€€å‡ºç : {status['return_code']}")
                print(f"  è¿è¡Œæ—¶é—´: {status['total_time_seconds']:.3f} ç§’")
                print(f"  GPUå®é™…ä½¿ç”¨: {'æ˜¯' if status.get('gpu_actually_used') else 'å¦'}")
                print("\n  è¿›ç¨‹å¯èƒ½åœ¨å¯åŠ¨é˜¶æ®µå°±å¤±è´¥äº†ï¼Œæ²¡æœ‰çœŸæ­£è¿è¡Œæ¨ç†ä»»åŠ¡ã€‚")
                print("  è¯·å…ˆè§£å†³ä¾èµ–é—®é¢˜ï¼ˆå¦‚ä¸Šé¢çš„numbaé”™è¯¯ï¼‰ï¼Œç„¶åé‡æ–°è¿è¡Œã€‚")
                print("=" * 70 + "\n")
        
        # å¦‚æœæœ‰æ£€æµ‹åˆ°çš„é”™è¯¯ï¼Œå…ˆæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if 'errors_detected' in results and results['errors_detected']:
            print("\n" + "=" * 70)
            print("âš ï¸  æ£€æµ‹åˆ°é”™è¯¯")
            print("=" * 70)
            for error in results['errors_detected']:
                print(f"\nã€{error['message']}ã€‘")
                if 'description' in error:
                    print(f"  {error['description']}")
                if 'suggestions' in error:
                    print("  å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                    for suggestion in error['suggestions']:
                        print(f"    â€¢ {suggestion}")
            print("\n" + "=" * 70)
        
        print("\n" + "=" * 70)
        print("è¯„æµ‹ç»“æœ")
        print("=" * 70)
        
        # æ€§èƒ½æŒ‡æ ‡
        print("\nã€æ€§èƒ½æŒ‡æ ‡ã€‘")
        perf = results['performance']
        print(f"  æ€»è¿è¡Œæ—¶é—´:     {perf['total_time_seconds']:.3f} ç§’")
        print(f"  å¤„ç†æ ·æœ¬æ•°:     {perf['processed_samples']} ä¸ª")
        print(f"  å¹³å‡ååé‡:     {perf['fps']:.2f} samples/s")
        print(f"  å¹³å‡æ¯æ ·æœ¬æ—¶é—´: {perf['time_per_sample_ms']:.2f} ms")
        
        # CPUä½¿ç”¨ç‡
        print("\nã€CPUä½¿ç”¨ç‡ã€‘")
        cpu = results['cpu']
        print(f"  å¹³å‡:           {cpu['mean_percent']:.2f}%")
        print(f"  å³°å€¼:           {cpu['max_percent']:.2f}%")
        print(f"  æœ€ä½:           {cpu['min_percent']:.2f}%")
        
        # å†…å­˜ä½¿ç”¨
        print("\nã€å†…å­˜ä½¿ç”¨ã€‘")
        mem = results['memory']
        print(f"  RSS (å®é™…ç‰©ç†å†…å­˜):")
        print(f"    å¹³å‡:         {mem['rss']['mean_mb']:.2f} MB")
        print(f"    å³°å€¼:         {mem['rss']['max_mb']:.2f} MB")
        print(f"    æœ€ä½:         {mem['rss']['min_mb']:.2f} MB")
        print(f"  VMS (è™šæ‹Ÿå†…å­˜):")
        print(f"    å¹³å‡:         {mem['vms']['mean_mb']:.2f} MB")
        print(f"    å³°å€¼:         {mem['vms']['max_mb']:.2f} MB")
        print(f"  å†…å­˜å ç”¨ç‡:")
        print(f"    å¹³å‡:         {mem['percent']['mean']:.2f}%")
        print(f"    å³°å€¼:         {mem['percent']['max']:.2f}%")
        
        # GPUä½¿ç”¨
        if 'gpu' in results:
            print("\nã€GPUä½¿ç”¨ã€‘")
            gpu = results['gpu']
            print(f"  GPUåˆ©ç”¨ç‡:")
            print(f"    å¹³å‡:         {gpu['utilization']['mean_percent']:.2f}%")
            print(f"    å³°å€¼:         {gpu['utilization']['max_percent']:.2f}%")
            print(f"    æœ€ä½:         {gpu['utilization']['min_percent']:.2f}%")
            print(f"  æ˜¾å­˜ä½¿ç”¨:")
            if 'total_mb' in gpu['memory']:
                print(f"    æ€»æ˜¾å­˜:       {gpu['memory']['total_mb']:.2f} MB")
            print(f"    å¹³å‡:         {gpu['memory']['mean_mb']:.2f} MB")
            print(f"    å³°å€¼:         {gpu['memory']['max_mb']:.2f} MB")
            print(f"    æœ€ä½:         {gpu['memory']['min_mb']:.2f} MB")
            print(f"  æ˜¾å­˜å ç”¨ç‡:")
            print(f"    å¹³å‡:         {gpu['memory_percent']['mean']:.2f}%")
            print(f"    å³°å€¼:         {gpu['memory_percent']['max']:.2f}%")
            
            # æ˜¾ç¤ºåŸºçº¿ä¿¡æ¯
            if 'baseline' in gpu:
                print(f"\n  GPUåŸºçº¿çŠ¶æ€:")
                print(f"    æ˜¾å­˜åŸºçº¿:     {gpu['baseline']['memory_mb']:.2f} MB")
                print(f"    åˆ©ç”¨ç‡åŸºçº¿:   {gpu['baseline']['utilization_percent']:.2f}%")
            
            # æ˜¾ç¤ºå¢é‡ï¼ˆå®é™…ä½¿ç”¨ï¼‰
            if 'memory_increment' in gpu:
                print(f"\n  æ˜¾å­˜å¢é‡ï¼ˆå®é™…ä½¿ç”¨ï¼‰:")
                print(f"    å¹³å‡:         {gpu['memory_increment']['mean_mb']:.2f} MB")
                print(f"    å³°å€¼:         {gpu['memory_increment']['max_mb']:.2f} MB")
                print(f"    æœ€ä½:         {gpu['memory_increment']['min_mb']:.2f} MB")
            
            if 'utilization_increment' in gpu:
                print(f"  GPUåˆ©ç”¨ç‡å¢é‡ï¼ˆå®é™…ä½¿ç”¨ï¼‰:")
                print(f"    å¹³å‡:         {gpu['utilization_increment']['mean_percent']:.2f}%")
                print(f"    å³°å€¼:         {gpu['utilization_increment']['max_percent']:.2f}%")
                print(f"    æœ€ä½:         {gpu['utilization_increment']['min_percent']:.2f}%")
            
            # æ˜¾ç¤ºåŠŸè€—å’Œæ¸©åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if 'power' in gpu:
                print(f"  GPUåŠŸè€—:")
                if 'baseline_watts' in gpu['power']:
                    print(f"    åŸºçº¿:         {gpu['power']['baseline_watts']:.2f} W")
                    if 'increment_mean_watts' in gpu['power']:
                        print(f"    å¢é‡å¹³å‡:     {gpu['power']['increment_mean_watts']:.2f} W")
                        print(f"    å¢é‡å³°å€¼:     {gpu['power']['increment_max_watts']:.2f} W")
                print(f"    å¹³å‡:         {gpu['power']['mean_watts']:.2f} W")
                print(f"    å³°å€¼:         {gpu['power']['max_watts']:.2f} W")
                print(f"    æœ€ä½:         {gpu['power']['min_watts']:.2f} W")
            if 'temperature' in gpu:
                print(f"  GPUæ¸©åº¦:")
                print(f"    å¹³å‡:         {gpu['temperature']['mean_c']:.2f} Â°C")
                print(f"    å³°å€¼:         {gpu['temperature']['max_c']:.2f} Â°C")
                print(f"    æœ€ä½:         {gpu['temperature']['min_c']:.2f} Â°C")
            
            # å¦‚æœGPUåˆ©ç”¨ç‡å˜åŒ–å¾ˆå°ï¼Œç»™å‡ºæç¤º
            util_range = gpu['utilization']['max_percent'] - gpu['utilization']['min_percent']
            if util_range < 5 and gpu['utilization']['mean_percent'] < 50:
                print(f"\n  ğŸ’¡ æç¤º: GPUåˆ©ç”¨ç‡å˜åŒ–è¾ƒå° ({util_range:.2f}%)ï¼Œå¯èƒ½åŸå› ï¼š")
                print(f"     - è¿›ç¨‹è¿è¡Œæ—¶é—´è¾ƒçŸ­ï¼ŒGPUä½¿ç”¨é«˜å³°æœŸè¢«é”™è¿‡")
                print(f"     - nvidia-smiçš„utilization.gpuæ˜¯è¿‡å»1ç§’çš„å¹³å‡å€¼ï¼Œå¯èƒ½ä¸å¤Ÿå®æ—¶")
                print(f"     - å»ºè®®ä½¿ç”¨æ›´çŸ­çš„é‡‡æ ·é—´éš”ï¼ˆ--sample-interval 0.01ï¼‰")
        else:
            print("\nã€GPUä½¿ç”¨ã€‘")
            print("  âš ï¸  æœªæ”¶é›†åˆ°GPUæ•°æ®ï¼ˆå¯èƒ½æœªä½¿ç”¨GPUæˆ–nvidia-smiä¸å¯ç”¨ï¼‰")
        
        # é‡‡æ ·ä¿¡æ¯
        print("\nã€ç›‘æ§ä¿¡æ¯ã€‘")
        print(f"  é‡‡æ ·æ¬¡æ•°:       {results['samples']}")
        print(f"  é‡‡æ ·é—´éš”:       {results['sample_interval_ms']} ms")
        
        # å¦‚æœé‡‡æ ·æ¬¡æ•°å¤ªå°‘ï¼Œç»™å‡ºè­¦å‘Š
        if results['samples'] < 10:
            print(f"  âš ï¸  è­¦å‘Š: é‡‡æ ·æ¬¡æ•°è¾ƒå°‘ï¼Œç»Ÿè®¡æ•°æ®å¯èƒ½ä¸å¤Ÿå‡†ç¡®")
            print(f"     å»ºè®®: å¤„ç†æ›´å¤šæ ·æœ¬ä»¥è·å¾—æ›´å‡†ç¡®çš„æ€§èƒ½æ•°æ®")
        
        print("\n" + "=" * 70)
    
    def save_results(self, results, output_file='benchmark_inference_results.json'):
        """ä¿å­˜è¯„æµ‹ç»“æœåˆ°æ–‡ä»¶"""
        # ç§»é™¤stdout/stderrä»¥å‡å°æ–‡ä»¶å¤§å°ï¼ˆå¯é€‰ï¼‰
        results_to_save = results.copy()
        if 'stdout' in results_to_save:
            results_to_save['stdout_length'] = len(results_to_save['stdout'])
            del results_to_save['stdout']
        if 'stderr' in results_to_save:
            results_to_save['stderr_length'] = len(results_to_save['stderr'])
            del results_to_save['stderr']
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        print(f"\nè¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨ç†è„šæœ¬èµ„æºè¯„æµ‹å·¥å…·')
    parser.add_argument('script', help='æ¨ç†è„šæœ¬è·¯å¾„ï¼ˆå¦‚ tools/test.pyï¼‰')
    parser.add_argument('config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('checkpoint', help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gpu-id', type=int, default=0,
                        help='GPU ID (é»˜è®¤: 0)')
    parser.add_argument('--eval', type=str, nargs='+',
                        help='è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ bboxï¼‰')
    parser.add_argument('--out', type=str,
                        help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--show', action='store_true',
                        help='æ˜¾ç¤ºç»“æœ')
    parser.add_argument('--show-dir', type=str,
                        help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--fuse-conv-bn', action='store_true',
                        help='èåˆconvå’Œbnå±‚')
    parser.add_argument('--sample-interval', type=float, default=0.1,
                        help='é‡‡æ ·é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤: 0.1ï¼‰')
    parser.add_argument('--output', default='benchmark_inference_results.json',
                        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: benchmark_inference_results.json)')
    parser.add_argument('--no-save', action='store_true',
                        help='ä¸ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('--additional-args', nargs=argparse.REMAINDER,
                        help='é¢å¤–çš„å‘½ä»¤è¡Œå‚æ•°ï¼ˆæ”¾åœ¨æœ€åï¼‰')
    parser.add_argument('--debug', action='store_true',
                        help='è°ƒè¯•æ¨¡å¼ï¼šå®æ—¶æ‰“å°GPUä½¿ç”¨æƒ…å†µ')
    
    args = parser.parse_args()
    
    # æ„å»ºé¢å¤–å‚æ•°åˆ—è¡¨
    additional_args = []
    if args.eval:
        additional_args.extend(['--eval'] + args.eval)
    if args.out:
        additional_args.extend(['--out', args.out])
    if args.show:
        additional_args.append('--show')
    if args.show_dir:
        additional_args.extend(['--show-dir', args.show_dir])
    if args.fuse_conv_bn:
        additional_args.append('--fuse-conv-bn')
    if args.additional_args:
        additional_args.extend(args.additional_args)
    
    try:
        # åˆ›å»ºè¯„æµ‹å·¥å…·
        benchmark = InferenceBenchmark(
            script_path=args.script,
            config_path=args.config,
            checkpoint_path=args.checkpoint,
            gpu_id=args.gpu_id,
            additional_args=additional_args,
            sample_interval=args.sample_interval,
            debug=args.debug
        )
        
        # è¿è¡Œè¯„æµ‹
        results = benchmark.run_benchmark()
        
        # æ‰“å°ç»“æœ
        benchmark.print_results(results)
        
        # ä¿å­˜ç»“æœ
        if not args.no_save:
            benchmark.save_results(results, args.output)
        
        # è¿”å›çŠ¶æ€ç 
        return results.get('return_code', 0)
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())

